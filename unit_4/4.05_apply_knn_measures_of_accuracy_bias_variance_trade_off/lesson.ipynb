{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4.5: Data modeling for accuracy\n",
    "\n",
    "### Lesson Duration: 3 hours\n",
    "\n",
    "> Purpose: The purpose of this lesson is to apply different models on the same data and compare the accuracies to select the best performing model. We will also talk a bit more in detail about different measures of accuracies for regression models and check how they are implemented in Python.\n",
    "\n",
    "<!-- ðŸš¨ðŸš¨ðŸš¨ @himanshu: do we need both unit4.csv and lesson_4.05_data.csv? it is confusing that notebook is using unit4.csv and the code example in lesson.md is using the other file.  -->\n",
    "\n",
    "### Setup\n",
    "\n",
    "- All previous set up\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "After this lesson, students will be able to:\n",
    "\n",
    "- Implement `KNN` (_k nearest neighbor_ algorithm)\n",
    "- Choose the best value of `k` for the `KNN`\n",
    "- Understand different measures of accuracies for regression models\n",
    "- Compare linear regression and `KNN` models and select better performing model\n",
    "\n",
    "---\n",
    "\n",
    "### Lesson 1 key concepts\n",
    "\n",
    "> :clock10: 20 min\n",
    "\n",
    "- Final preparation of data for fitting the model\n",
    "\n",
    "      - Putting numerical and categorical encoded data together\n",
    "      - Train test split\n",
    "\n",
    "- Revisit `KNN`\n",
    "- Implement `KNN`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Sample:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4#:~:text=K%2Dnearest%20neighbors%20(KNN),closet%20to%20the%20test%20data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('./files_for_lesson_and_activities/lesson_4.05_data.csv') # this file is in files_for_lesson_and_activities folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EB885UN\\AppData\\Local\\Temp\\ipykernel_31304\\1065154470.py:2: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  categoricals = data.select_dtypes(np.object)\n"
     ]
    }
   ],
   "source": [
    "numericals = data.select_dtypes(np.number)\n",
    "categoricals = data.select_dtypes(np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HV1</th>\n",
       "      <th>IC1</th>\n",
       "      <th>IC5</th>\n",
       "      <th>CARDPROM</th>\n",
       "      <th>NUMPRM12</th>\n",
       "      <th>NGIFTALL</th>\n",
       "      <th>TIMELAG</th>\n",
       "      <th>year</th>\n",
       "      <th>RFA_frequency</th>\n",
       "      <th>AVGGIFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>479</td>\n",
       "      <td>307</td>\n",
       "      <td>12883</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7.741935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HV1  IC1    IC5  CARDPROM  NUMPRM12  NGIFTALL  TIMELAG  year  \\\n",
       "0  479  307  12883        27        14        31      4.0  37.0   \n",
       "\n",
       "   RFA_frequency   AVGGIFT  \n",
       "0              4  7.741935  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericals.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = numericals['AVGGIFT']\n",
    "numericals = numericals.drop('AVGGIFT', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>HOMEOWNR</th>\n",
       "      <th>DOMAIN_letters</th>\n",
       "      <th>RFA_monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>T</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  GENDER HOMEOWNR DOMAIN_letters RFA_monetary\n",
       "0      F    other              T            E"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricals.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = StandardScaler().fit(numericals)\n",
    "x_standardized = transformer.transform(numericals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(handle_unknown='error', drop='first').fit(categoricals)\n",
    "encoded = encoder.transform(categoricals).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((x_standardized, encoded), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "model = KNeighborsRegressor(n_neighbors=4)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_knn = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21818365133023454"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.05 Activity 1\n",
    "\n",
    "Refer to the `files_for_activities/lesson_4.05_data.csv` file.\n",
    "\n",
    "1. Fit a linear regression model on the same processed data. Note we have already performed basic cleaning operations, data preprocessing - scaling and encoding, and then the train test split. Fit the model on the training data directly.\n",
    "2. Find the accuracy of the model using the test data.\n",
    "3. Compare the accuracies of linear regression and the `KNN` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4740230878370695"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "\n",
    "#para hacer el predict hay que hacer antes el fit (entreno). Si no entreno no hay predict que valga\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 2 key concepts\n",
    "\n",
    "> :clock10: 20 min\n",
    "\n",
    "- Choosing the best value of `k` for the `KNN`\n",
    "\n",
    "Code Sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [08:20<00:00, 55.58s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from tqdm import tqdm\n",
    "scores = []\n",
    "for k in tqdm(range(5,14)):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdashed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkerfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy scores vs. K Value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:2757\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   2758\u001b[0m         \u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39mscalex, scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   2759\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1632\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py:498\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (8,) and (9,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(5,14),scores,color = 'blue', linestyle='dashed',\n",
    "         marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('accuracy scores vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~4.05 Activity 2~~\n",
    "\n",
    "If you think a little bit about it, the number of neighbors might be very important for our results, but will it be the only parameter that matters? Go to the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and check the parameters and the values they can take, pick the one you think is more relevant and change its value in the model.\n",
    "**Hint**: If `K` (number of neighbors) is the most important one, maybe we could measure the way these `K` instances affect our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 3 key concepts (ppt added)\n",
    "\n",
    "> :clock10: 20 min\n",
    "\n",
    "- [**SLIDES**](https://docs.google.com/presentation/d/1REBc9LkwY1lopuMeeG7nOgJh5yqTxWJh04rCov80qmM/edit?usp=sharing)\n",
    "\n",
    "- Bias\n",
    "- Variance\n",
    "- Bias/Variance trade-off\n",
    "- Over-fitting vs. under-fitting\n",
    "- Discussion on Over-fitted models vs. under-fitted models\n",
    "\n",
    "      - How they can impact the predictions\n",
    "\n",
    "Description:\n",
    "\n",
    "- **Bias** - When we say that a measurement is unbiased, we mean that the average of a large set of observations will be close to the true value.\n",
    "\n",
    "- **Variance** is a measurement of the spread between numbers in a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~4.05 Activity 3~~\n",
    "\n",
    "Let's visualize how `KNN` actually works. First of all install the [`mlxtend` library](http://rasbt.github.io/mlxtend/) and create a dataframe containing the two most relevant **numerical** variables and the target, in **that order**. Once you have done it sample it with `n = 100`, introduce that sample into this function with an arbitrary `k`:\n",
    "\n",
    "```python\n",
    "def knn_comparison(data, k):\n",
    "    x = data.iloc[:, 0:2].values\n",
    "    y = data.iloc[:, -1].astype(int).values\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(x, y)\n",
    "\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plot_decision_regions(x, y, clf=knn)\n",
    "    plt.title(\"Knn with K=\"+ str(k), fontsize = 18)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "- What can you see in the plot? Now try to create a function `plot_knn_boundaries` to loop over the previous function and iterate over the `ks = [1, 3, 5, 10, 25, 50]`. And now, _can you tell the difference between the plots_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c conda-forge mlxtend\n",
    "\n",
    "`Ã³`\n",
    "\n",
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_knn = numericals[[\"HV1\", \"NGIFTALL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_knn.loc[:, 'AVGGIFT'] = y.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_knn.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_comparison(data, k):\n",
    "    x = data.iloc[:, 0:2].values\n",
    "    y = data.iloc[:, -1].astype(int).values\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(x, y)\n",
    "\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plot_decision_regions(x, y, clf=knn)\n",
    "    plt.title(\"Knn with K=\"+ str(k), fontsize = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knn_boundaries(data, ks = [1, 3, 5, 10, 25, 50]):\n",
    "    for i in tqdm(ks):\n",
    "        knn_comparison(data, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_knn = data_knn.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_boundaries(data_knn, ks = [1, 3, 5, 10, 25, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The lower the number of `k` the more over-fitted it will be. We can see that with `k = 1`, the boundaries are very clear and as we increase `k` the plots start turning very messy until the last two plots, where it is oversimplified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 4 key concepts\n",
    "\n",
    "> :clock10: 20 min\n",
    "\n",
    "- Measures of accuracies for regression models\n",
    "\n",
    "      - `MAE` (Mean Absolute Error)\n",
    "      - `MSE` (Mean Square Errors) and `RMSE` (Root Mean Square Errors)\n",
    "      - `R` square\n",
    "      - Adjusted `R` square\n",
    "\n",
    "Note: Adding more features to the model might tend to increase the `r` square even if the features are not significant while the same doesn't happen with adjusted `r` square. Adding insignificant features to the model might also lead to over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "- In regression problems, we measure the accuracy of an algorithm based on how far away the values that the algorithm predicts are from the true values.\n",
    "\n",
    "![Measure of Accuracy](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/4.5-measures_of_accuracies.png)\n",
    "\n",
    "Code Sample: `MAE`\n",
    "\n",
    "The **mean absolute error** is the average (_mean_) of the absolute value of the distance between predicted and actual values. It is a measure of absolute error: because of this, we do not know if the algorithm is overestimating or underestimating when it is incorrect. Also because of the absolute value, it is not very sensitive to outliers, comparatively to other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "score = mean_absolute_error(actual_values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Sample: `MSE` \n",
    "\n",
    "The **`MSE`** takes the difference between the predicted value and the actual value, squares it, then takes the average (_mean_) of those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train,y_train)\n",
    "predictions  = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Sample: RMSE\n",
    "\n",
    "The **`RMSE`** is the square root of the _mean square error_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = math.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Sample: `R` square\n",
    "\n",
    "The **`R-squared`** is the proportion of the variance in the model predictions that is predictable based on the input values. Practically, it is a measure of how likely future samples are to be predicted accurately by the model.\n",
    "Adding more independent variables to a regression model tends to increase the `R-squared` value, which might give us a false idea about the accuracy of the model. This would also lead to over-fitting and can return an unwarranted high `R-squared` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r_squared = r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: `Adjusted R square`\n",
    "\n",
    "We use the **adjusted R-squared** to compare regression models that contain different numbers of independent variables. For example, there is a model with 10 variables and the other model has 20 variables. The model with 20 variables will give a larger R square but does that really mean that this model is better than the other model. We can't say it for sure. For this, we use adjusted the `r` square.\n",
    "\n",
    "Adjusted `r` square adjusts for the number of terms in the model. Its value increases only when the new term improves the model fit while it decreases when the term doesn't improve the model fit by a sufficient amount.\n",
    "\n",
    "![Adjusted R square](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/4.5-adjusted_r_square.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Sample: Adjusted R square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 1 - (1-r_squared)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.05 Activity 4\n",
    "\n",
    "- From all the regression metrics we have seen, which one do you think is the one to use in most cases?\n",
    "- Calculate and plot `R2`, `MSE`, `RMSE`, and `MAE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen **R2**, **MSE**, **RMSE** and **MAE**. Of course, there is not a magic solution for which you should always use it, but there are some details worth knowing:\n",
    "\n",
    "- **R2** is scaled, which means that it is independent of the data. This one would be the one to go with if we don't know a lot about the data and general information about our model. However, it can be misleading, as it is supposed to be between **0** and **1** but sometimes **it is not** (you can read about it [here](http://www.fairlynerdy.com/what-is-r-squared/). In fact, **R2** is a **biased** estimator (more information [here](https://statisticsbyjim.com/regression/r-squared-too-high/#:~:text=Reason%201%3A%20R-squared%20is%20a%20biased%20estimate&text=In%20statistics%2C%20a%20biased%20estimator,use%20adjusted%20R2%20instead).\n",
    "\n",
    "- **MAE** would be the _median_ of the regression metrics as what it measures is the sum of distances between predicted and real values (errors), and that won't give a special treat to really bad predictions, so if that's what we want this metric should do great.\n",
    "\n",
    "  - _MSE_ - It is the mean of the squared distance of the errors, which will weight the bad predictions.\n",
    "  - _RMSE_ - Root _MSE_, essentially it is the same but it is easier to understand within the data context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=100)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2:', metrics.r2_score(y_test, y_pred))\n",
    "print('Adjusted R:',  1 - (1-metrics.r2_score(y_test, y_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Comparing regression models\n",
    "\n",
    "\n",
    "For this lab, we will be using the same dataset we used in the previous labs. We recommend using the same notebook since you will be reusing the same variables you previous created and used in labs. \n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. In this final lab, we will model our data. Import sklearn `train_test_split` and separate the data.\n",
    "2. Try a simple linear regression with all the data to see whether we are getting good results.\n",
    "3. Great! Now define a function that takes a list of models and train (and tests) them so we can try a lot of them without repeating code.\n",
    "4. Use the function to check `LinearRegressor` and `KNeighborsRegressor`.\n",
    "5. You can check also the `MLPRegressor` for this task!\n",
    "6. Check and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df.drop(columns = \"total_claim_amount\"),\n",
    "                                                    final_df.total_claim_amount, test_size = 0.2)\n",
    "```\n",
    "\n",
    "```python\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "LR.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "```python\n",
    "def models_automation(models, X_train, y_train, X_test, y_test):\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        print(f\"{model.__class__.__name__}: Train -> {model.score(X_train, y_train)}, Test -> {model.score(X_test, y_test)}\")\n",
    "\n",
    "linear_models = [LinearRegression(), Lasso(), Ridge(), ElasticNet()]\n",
    "models_automation(linear_models, X_train, y_train) \n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knnr = [KNeighborsRegressor(5)]\n",
    "models_automation([knnr], X_train, y_train)\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlpr = [MLPRegressor(max_iter = 1000)]\n",
    "models_automation([mlpr], X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "- [Choosing the correct regression analysis method](https://statisticsbyjim.com/regression/choosing-regression-analysis/)\n",
    "- [Continuous, discrete and categorical variables](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/what-are-categorical-discrete-and-continuous-variables/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
